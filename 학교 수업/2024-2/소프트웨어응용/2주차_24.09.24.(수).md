<h1>소프트웨어응용</h1>

<h2>CNN & RNN</h3>

<h3>01. 딥러닝 학습</h3>

- 신경망 학습
    - 기계적 관점에서 학습이란, 일련의 반복적인 과정을 통해 출력하는 결과값이 점진적으로 원하는 기댓값에 수렴하도록 변하는 것을 의미함
    - 신경망 학습은 다음과 같은 과정의 반복으로 이루어짐
    <br>1. 전방 계산
    <br>2. 오차 계산
    <br>3. 후방 가중치 계산

- 퍼셉트론
    - Visible input layer
    - Multi Layer Perceptron(MLP) / Hidden Layers
    - Visible output layer

<h3>02. CNN</h3>

-  CNN: Convolution Neural Network
    - 컨볼루션 신경망은 딥러닝(컴퓨터비전)에서 가장 성공한 모델
    - ..이었는데, 현재는 Transformer 모델이 우세하다.
        - GPT : Generated Pre-trained Transformer

    - (a) 자율주행차
    - (b) 딥드림
    - (c) 영상 주석달기
    - (d) 병변 부위 찾기
    - 컨볼루션 연산을 활용한 NN
    - 컨볼루션 연산 : 파란 부분(receptive field; 수용장)과 커널(필터)의 선형 결합

- Why convolution?
    - 컨볼루션 연산을 통해서 입력(이미지)의 특징을 추출 가능
    - 수평 에지 커널 ➡️ 수평 에지 특징 맵 | 가로선 부각
    - 수직 에지 커널 ➡️ 수직 에지 특징 맵 | 세로선 부각

- 개와 고양이 사진을 구별하는 문제
    - 예전 컴퓨터 비전에서는 매우 어려운 문제여서 캡차로 사용
        - 랜덤하게 찍는 경우 (0.5)^12=1/4096 확률로 뚫림(12개 영상 사용하는 경우)
        - 고전적인 컴퓨터 비전 기술로 82.7% 정확률 달성. (0.827)^12=1/9.77 확률로 뚫림

- 컨볼루션 신경밍
    - 98.914% 달성. (0.98914)^12 = 0.87719 확률로 뚫림
    - 캡차로서 기능 상실

- ILSVRC2012 우승한 AlexNet
    - 자연 영상을 1000개 부류로 분류하는 문제
    - AlexNet은 오류율 15.3%로 우승(이전 해 대회는 고전적인 컴퓨터 비전 기술로 25.8%가 우승)
    - 이후 대부분 컴퓨터 비전 연구자가 고전적 방법에서 딥러닝으로 전환

- 2D Convolutions | 배열이 2차원이라 2D Convolution이 아니라 배열의 움직임이 2D이기 때문에
    - 입력 사이즈 : 5 * 5
    - 커널 사이즈 : 3 * 3
    - 출력 사이즈 : 3 * 3

- output 줄이고 싶지 않을 수도 있다
- 가장자리에 있는 것들 참여 줄어들 수 있다

- Padding
    - 여분 .. 원래 1번 참여하던 거 몇 번 더 참여 가능 ..
- Stride
    - 몇 칸 움직일 거냐?
    - 앞까지는 1번씩만 움직였다 ..
    - 대충보고 띄엄띄엄 .. Stride 늘리기
- 출력 크기 계산
    - (H, W) : 입력 크기 (input size)
    - (FH, FW) : 필터 크기 (filter/kernel size)
    - S : 스트라이드 (stride)
    - P : 패딩 (padding)
    - (OH, OW) : 출력 크기 (output size)
    - 수식 외우는 거 시험 X ..

- RGB 이미지 입력은 3차원 (W x H x C)
    - 입력의 채널 수와 커널의 채널 수가 같아야 함
        - 입력 사이즈 : 6 * 6 * 3
        - 커널 사이즈 : 3 * 3 * 3
        - 출력 사이즈 : 4 * 4

        - 커널 개수 : 2
        - 최종 출력 사이즈 : 4 * 4 * 2

    - 같은 차원끼리 계산 .. 각 위치/곱 ➡️ 합

- Pooling layer
    - max 풀링은 커널 안에 있는 화소 중에 최대값을 취함(average 풀링은 평균 취함)
    - 특징 맵에 있는 지나친 상세함을 줄여 요약 통계량을 추출해 줌
    - Stride를 s로 하면 특징 맵은 s배만큼 줄어듦 (길이적으로 .. 넓이로 따지면 s^2배)
    - Pooling(downsampling)이 필요한 이유는, featuremap의 weight parameter 갯수를 줄이기 위함
    - 또한 Pooling을 사용하면 연속적인 ConvNet층이 점점 커지는 window를 보도록 만들어 (receptive field를 넓힘) 필터의 공간적인 계층구조를 형성하는데 도움을 줄 수 있다.

- Convolutional Block
- AlexNet Architecture

<h3>03. RNN</h3>

- 시계열 데이터
    - 시간 정보가 들어 있는 데이터
        - 예) 문장 "세상에는 시계열 데이터가 참 많다"
        - 단어가 나타나는 순서가 중요
        - 샘플의 길이가 다름
- 시계열 데이터를 인식하는 고전적인 모델
    - ARIMA(autoregressive integrated moving average)와 SARIMA(seasonal ARIMA)
    - Prophet 등
- 시계열 데이터를 인식하는 딥러닝 모델
    - 순환 신경망recurrent neural network(RNN)
    - LSTM(long short-term memory), GRU(Gated Recurrent Unit)
- 시계열 데이터
    - 시간 축을 따라 신호가 변하는 동적 데이터
    - 앞에서 공부한 다층 퍼셉트론, 깊은 다층 퍼셉트론, 컨볼루션 신경망
- 딥러닝에서는 시계열 특성을 반영하는 순환 신경망 또는 LSTM 활용
- 시계열 데이터의 독특한 특성
    - 요소의 순서가 중요
        - 예) "고양이를 따라가는 강아지"를 "강아지를 따라가는 고양이"으로 바꾸면 의미 훼손
    - 샘플의 길이가 다름
        - 예) 짧은 발음 "인공지능"과 긴 발음 "인~공~~지~능"
    - 문맥 의존성
        - 예) "시계열은 앞에서 말한 바와 ... 특성이 있다"에서 "시계열은"과 "특성이 있다"는 밀접한 관련성
    - 계졀성
        - 예) 상추 판매량, 미세먼지 수치, 항공권 판매량 등
- 시계열 데이터의 표현
    - 가변 길이고 벡터의 벡터임
        - 예) 매일 기온, 습도, 미세먼지 농도를 기록한다면, a1=(23.5,42,0.1), a2=(25.5, 45.0, 0.08), ...

- 순환 신경망은 유여한 구조라 여러 문제에 적용 가능
    - 대표적 응용은 미래 예측(prediction 또는 forecasting)
        - 내일 주가 예측
        - 내일 날씨 예측
        - 기계의 고장 예측
        - 풍속과 풍향 예측(풍력 발전기의 효율 향상)
        - 농산물 가격/수요량 예측 등
    - 언어 번역에 응용
    - 음성 인식에 응용
    - 생성 모델에 응용(예, 사진을 보고 설명 문장 생성)

- RNN 구조
    - 가중치 공유
        - 순간마다 서로 다른 가중치를 가지는 것이 아님
        - 모든 순간이 { U, V, W }를 공유

- RNN 활용
    - 일대다(one-to-many)
    - 다대일(many-to-one)
    - 다대다(many-to-many)

- LSTM(Long Short Term Memory)
    - CNN은 성능 좋고 RNN은 성능 별로다.
    - GRU나 LSTM의 형태로 쓰인다.
    - 기본 RNN은 vanishing gradient problem 발생
        - 관련 정보와 그 정보를 사용하는 지점 사이 거리가 멀 경우 RNN 학습능력 저하
    - LSTM은 RNN의 hidden state에 cell-state를 추가한 구조
    - Cell State는 hidden state와 마찬가지로 이전 시점의 cell state를 다음 시점으로 넘겨준다.
    - Cell state의 주 역할은 gate(forget gate, input gate, output gate)들과 함께 작용하여 정보를 선택적으로 활용할 수 있도록 하는 것

- GRU(gated Recurrent Unit)
    - LSTM을 간소화한 모델
        - GRU에서는 reset gate, update gate 2개의 gate만을 사용

<h3>04. 학습 구현</h3>

